[
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "In the effort to understand the brain and its function, connectomics has proven a useful tool.  Connectomes are now available for entire larval (Winding et al. 2023) and adult fly brains (Schlegel et al. 2023; Dorkenwald et al. 2023), large regions of mouse visual cortex (MICrONS Consortium et al. 2021), human cortex (Shapson-Coe et al. 2021), and many other areas and organisms. The largest of these connectome reconstructions now contain on the order of hundreds of thousands of neurons, and hundreds of millions of synapses. This unprecedented scale in connnectome reconstruction has been enabled by segmenting cells and annotating synapses with computer vision, followed by laborious human proofreading to correct false merges and extend incomplete pieces of neurons. This latter effort has been done, understandably, with the intent that we want to be able to trust the resulting reconstruction and believe that it is a fair representation of the neuroanatomy and connectivity of the biological sample: thus, one should correct the results of an automated algorithm by this manual proofreading process.Connectome: a map of neural wiring diagrams in precise detail, often obtained via electron microscopy followed by manual or automated reconstruction of the neurons and other cells in that image volume.\n\n\n\nHowever, since this manual proofreading is laborious (for instance, there were 2.7 million edits in FlyWire over 4 years as of June 2023) it is worth reflecting on the scientific value of this effort. How consequential was this proofreading on the downstream measures we care about for analysis, such as connection probabilities between cell types? Are some connectivity features stable with proofreading, and others variable? Are certain edits more important than others, in terms of their impact on overal circuit structure? Next, we touch on how others have explored these and related questions in prior work, before motivating our own contributions here.\n\n\n\nIn Priebe, Vogelstein, and Bock (2013), the authors ask whether one should desire lots of “highly proofread” data, or a smaller amount of partially proofread or noisy data. They frame this question from the perspective of statistical hypothesis testing, asking which approach has higher power for distinguishing network parameters under a toy model. This work is an illustrative case for why, at least in theory, it is possible to actually prefer lots of unproofread data to a smaller amount of curated data. This is a prospect worth considering, especially in light of costly proofreading endeavors. However, this work was done in a toy model which was set up such that unproofread data had no bias, but merely represented a noisier version of a ground truth network.  In reality, the unproofread networks we observe in connectomics likely have extreme bias in the way that errors are observed. For instance, one error in segmentation could lead to missing the entire primary axon of a neuron, effectively deleting all of that neurons outputs in one stroke. In this work, we consider how connectivity estimandsBias: the distance between an expected value of an estimator and the true value of the parameter it is trying to estimate.\n\n\n\nText describing the key result from Priebe, Vogelstein, and Bock (2013). In essence, power is greater in their toy example when tracing more edges with more errors.\n\n\nThis question of technical variability and proofreading tangentially arose in Schlegel et al. (2023) as the authors examined a comparison of two fly connectomes: FlyWire and Hemibrain. The authors were interested in quantitative comparisons of connectivity between cell types that were cross-matched between the left and right hemispheres of the FlyWire connectome and the single hemisphere Hemibrain connectome. They found a general agreement in cell-type-to-cell-type connections both within (FlyWire-left:FlyWire-right) and between (FlyWire:Hemibrain) datasets; still, they wondered how much technical differences in reconstruction between these reconstructions could explain the differences they saw. The authors created synthetic datasets by\n\ncasey quantitative neuroanatomy paper\n\n\n\n\nThere are several reasons why understanding the variability of connectivity with respect to proofreading is worthwhile for modern connectomics:\nFirst, understanding the sensitivity of connectivity features to proofreading provides clues as to the technical variability associated with an imperfect reconstruction of an underlying biological network. For instance, if one wanted to compare connectivity feature \\(Y\\) between two datasets, but \\(Y\\) varies drastically based on proofreading level, then it will be challenging to compare \\(Y\\) without ensuring proofreading was done in the exact same way for these two datasets.\nSecond, understanding this sensitivity is important for directing future proofreading efforts. Achieving a “perfect connectome” reconstruction is a comforting goal to strive for, but given finite time and resources, it is worthwhile to consider which efforts will actually affect downstream analysis we are interested in. Increasingly, a single connectomics datasets may be used for addressing many questions by directing proofreading toward particular sub-goals. It is worth understanding which questions can be answered “out-of-the-box” with no additional proofreading after the initial segmentation, and which will require further effort to get a trustworth answer.\n\n\n\nIn this work, we investigate these questions using large-scale reconstructions of mouse visual cortex as testbeds for these ideas. We do not aim to answer these questions holistically for all connectomics datasets and possible connectivity features; such an endeavor would be impossible since the answer is likely to depend greatly on the dataset and the question. Rather, we provide a set of guiding principles for which types of features are likely to be variable, and provide tools for other connectomicists to analyze these questions on their own data.\n\n\n\nWe start by clearly defining a set of connectivity features to consider for this work.\nThen, we extract the complete set of proofreading edits performed on a highly curated column of cortical connectivity in a mouse’s visual cortex. We also map each synapse between neurons in this column to the edits it “depends” on, in the sense that those edits had to happen for a statement about soma \\(i\\) to soma \\(j\\) connectivity to be made.\nUsing this information, we then construct various “sloppy” networks, instantiated by replaying particular edits over the network and omitting others. These networks vary in their degree of completeness (i.e. some measure of how many of the edits were used) from the original segmentation all the way to the final proofread dataset. We also vary the “proofreader model” used to assign edits to “ON” or “OFF” in some ordering (i.e. whether edits are replayed uniformly at random, in the order they happened, in order of euclidean distance from the soma, etc.)."
  },
  {
    "objectID": "introduction.html#background",
    "href": "introduction.html#background",
    "title": "Introduction",
    "section": "",
    "text": "In the effort to understand the brain and its function, connectomics has proven a useful tool.  Connectomes are now available for entire larval (Winding et al. 2023) and adult fly brains (Schlegel et al. 2023; Dorkenwald et al. 2023), large regions of mouse visual cortex (MICrONS Consortium et al. 2021), human cortex (Shapson-Coe et al. 2021), and many other areas and organisms. The largest of these connectome reconstructions now contain on the order of hundreds of thousands of neurons, and hundreds of millions of synapses. This unprecedented scale in connnectome reconstruction has been enabled by segmenting cells and annotating synapses with computer vision, followed by laborious human proofreading to correct false merges and extend incomplete pieces of neurons. This latter effort has been done, understandably, with the intent that we want to be able to trust the resulting reconstruction and believe that it is a fair representation of the neuroanatomy and connectivity of the biological sample: thus, one should correct the results of an automated algorithm by this manual proofreading process.Connectome: a map of neural wiring diagrams in precise detail, often obtained via electron microscopy followed by manual or automated reconstruction of the neurons and other cells in that image volume."
  },
  {
    "objectID": "introduction.html#the-questions",
    "href": "introduction.html#the-questions",
    "title": "Introduction",
    "section": "",
    "text": "However, since this manual proofreading is laborious (for instance, there were 2.7 million edits in FlyWire over 4 years as of June 2023) it is worth reflecting on the scientific value of this effort. How consequential was this proofreading on the downstream measures we care about for analysis, such as connection probabilities between cell types? Are some connectivity features stable with proofreading, and others variable? Are certain edits more important than others, in terms of their impact on overal circuit structure? Next, we touch on how others have explored these and related questions in prior work, before motivating our own contributions here."
  },
  {
    "objectID": "introduction.html#prior-work",
    "href": "introduction.html#prior-work",
    "title": "Introduction",
    "section": "",
    "text": "In Priebe, Vogelstein, and Bock (2013), the authors ask whether one should desire lots of “highly proofread” data, or a smaller amount of partially proofread or noisy data. They frame this question from the perspective of statistical hypothesis testing, asking which approach has higher power for distinguishing network parameters under a toy model. This work is an illustrative case for why, at least in theory, it is possible to actually prefer lots of unproofread data to a smaller amount of curated data. This is a prospect worth considering, especially in light of costly proofreading endeavors. However, this work was done in a toy model which was set up such that unproofread data had no bias, but merely represented a noisier version of a ground truth network.  In reality, the unproofread networks we observe in connectomics likely have extreme bias in the way that errors are observed. For instance, one error in segmentation could lead to missing the entire primary axon of a neuron, effectively deleting all of that neurons outputs in one stroke. In this work, we consider how connectivity estimandsBias: the distance between an expected value of an estimator and the true value of the parameter it is trying to estimate.\n\n\n\nText describing the key result from Priebe, Vogelstein, and Bock (2013). In essence, power is greater in their toy example when tracing more edges with more errors.\n\n\nThis question of technical variability and proofreading tangentially arose in Schlegel et al. (2023) as the authors examined a comparison of two fly connectomes: FlyWire and Hemibrain. The authors were interested in quantitative comparisons of connectivity between cell types that were cross-matched between the left and right hemispheres of the FlyWire connectome and the single hemisphere Hemibrain connectome. They found a general agreement in cell-type-to-cell-type connections both within (FlyWire-left:FlyWire-right) and between (FlyWire:Hemibrain) datasets; still, they wondered how much technical differences in reconstruction between these reconstructions could explain the differences they saw. The authors created synthetic datasets by\n\ncasey quantitative neuroanatomy paper"
  },
  {
    "objectID": "introduction.html#why-it-matters",
    "href": "introduction.html#why-it-matters",
    "title": "Introduction",
    "section": "",
    "text": "There are several reasons why understanding the variability of connectivity with respect to proofreading is worthwhile for modern connectomics:\nFirst, understanding the sensitivity of connectivity features to proofreading provides clues as to the technical variability associated with an imperfect reconstruction of an underlying biological network. For instance, if one wanted to compare connectivity feature \\(Y\\) between two datasets, but \\(Y\\) varies drastically based on proofreading level, then it will be challenging to compare \\(Y\\) without ensuring proofreading was done in the exact same way for these two datasets.\nSecond, understanding this sensitivity is important for directing future proofreading efforts. Achieving a “perfect connectome” reconstruction is a comforting goal to strive for, but given finite time and resources, it is worthwhile to consider which efforts will actually affect downstream analysis we are interested in. Increasingly, a single connectomics datasets may be used for addressing many questions by directing proofreading toward particular sub-goals. It is worth understanding which questions can be answered “out-of-the-box” with no additional proofreading after the initial segmentation, and which will require further effort to get a trustworth answer."
  },
  {
    "objectID": "introduction.html#our-contribution",
    "href": "introduction.html#our-contribution",
    "title": "Introduction",
    "section": "",
    "text": "In this work, we investigate these questions using large-scale reconstructions of mouse visual cortex as testbeds for these ideas. We do not aim to answer these questions holistically for all connectomics datasets and possible connectivity features; such an endeavor would be impossible since the answer is likely to depend greatly on the dataset and the question. Rather, we provide a set of guiding principles for which types of features are likely to be variable, and provide tools for other connectomicists to analyze these questions on their own data."
  },
  {
    "objectID": "introduction.html#outline",
    "href": "introduction.html#outline",
    "title": "Introduction",
    "section": "",
    "text": "We start by clearly defining a set of connectivity features to consider for this work.\nThen, we extract the complete set of proofreading edits performed on a highly curated column of cortical connectivity in a mouse’s visual cortex. We also map each synapse between neurons in this column to the edits it “depends” on, in the sense that those edits had to happen for a statement about soma \\(i\\) to soma \\(j\\) connectivity to be made.\nUsing this information, we then construct various “sloppy” networks, instantiated by replaying particular edits over the network and omitting others. These networks vary in their degree of completeness (i.e. some measure of how many of the edits were used) from the original segmentation all the way to the final proofread dataset. We also vary the “proofreader model” used to assign edits to “ON” or “OFF” in some ordering (i.e. whether edits are replayed uniformly at random, in the order they happened, in order of euclidean distance from the soma, etc.)."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "docs",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "test.html",
    "href": "test.html",
    "title": "test",
    "section": "",
    "text": "test"
  },
  {
    "objectID": "abstract.html",
    "href": "abstract.html",
    "title": "Abstract",
    "section": "",
    "text": "Historically, connectomes have been extremely labor-intensive to generate, requiring vast amounts of person-hours to annotate electron microscopy (EM) image volumes. Automated reconstruction from EM images using computer vision has recently developed to the point where it is being deployed widely to reconstruct connectomes at unprecedented scales. However, these automated techniques, while impressive, typically still have many errors that require human proofreading of these datasets, which involves linking together fragments of the same neuron which have been separated or separating false-merges of distinct cells. Here, we study the effect that these proofreading modifications have on the cortical wiring diagram of a region of mouse visual cortex. We compute several connectivity metrics such as the probability of two cell types or two specific neurons connecting, and study how this connectivity changes as we artificially replay a specific subset of edits onto the neurons in the volume. We show that while X changes drastically with proofreading, Z and Y metrics are relatively stable after a key subset of edits are applied. Our work lays the foundation for a quantitative assessment of which areas for human or machine proofreading will be the most influential for downstream analysis, guiding future connectomics reconstructions towards answering scientific questions with a practical amount of effort. We also describe how our analysis reveals quantitative estimates of one aspect of the variability associated with connectome reconstruction."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  }
]