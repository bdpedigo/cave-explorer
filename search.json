[
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "In the effort to understand the brain and its function, connectomics (the mapping of neural wiring diagrams in precise detail, often via electron microscopy) has proven a useful tool. Connectomes are now available for entire larval (Winding et al. 2023) and adult fly brains (Schlegel et al. 2023; Dorkenwald et al. 2023), large regions of mouse visual cortex (MICrONS Consortium et al. 2021), human cortex (Shapson-Coe et al. 2021), and many others. These reconstructions now contain on the order of 100,000s of neurons, and 100s of millions of synapses. This dramatic scaling in accurate connnectome reconstruction has been enabled by a strategy of scalable automatic segmentation of neurons and detection of synapses, followed by laborious human proofreading to correct false merges and extend incomplete pieces of neurons. This effort has been done, understandably, with the intent that we want to be able to trust the resulting reconstruction and believe that it is a fair representation of the neuroanatomy and connectivity of the biological sample.\n\n\n\nHowever, since this manual proofreading is laborious (for instance, there were 2.7 million edits in FlyWire over 4 years as of June 2023) it is worth reflecting on the scientific value of this effort. How consequential was this proofreading on the downstream measures we care about for analysis, such as connection probabilities between cell types? Are some connectivity features stable with proofreading, and others variable? Are certain edits more important than others, in terms of their impact on overal circuit structure? Next, we touch on how others have explored these and related questions in prior work, before motivating our own contributions here.\n\n\n\n\npaper from carey and jovo on power\n\nIn Priebe, Vogelstein, and Bock (2013), the authors ask whether one should desire lots of “highly proofread” data, or a smaller amount of partially proofread or noisy data. They frame this question from the perspective of statistical hypothesis testing, asking which approach has higher power for distinguishing network parameters under a toy model. This work is an illustrative case for why, at least in theory, it is possible to actually prefer lots of unproofread data to a smaller amount of curated data. This is a prospect worth considering, especially in light of costly proofreading endeavors. However, in this work the toy model was set up such that unproofread data had no bias, but merely represented a noisier version of a ground truth network. In reality, the unproofread networks we observe in connectomics likely have extreme bias in the way that errors are observed. For instance, one error in segmentation could lead to missing the entire primary axon of a neuron, effectively deleting all of that neurons outputs in one stroke.\n\ncasey paper\nphillip paper mentions variability\n\n\n\n\nThere are several reasons why understanding the variability of connectivity with respect to proofreading is worthwhile for modern connectomics:\nFirst, understanding the sensitivity of connectivity features to proofreading provides clues as to the technical variability associated with an imperfect reconstruction of an underlying biological network. For instance, if one wanted to compare connectivity feature Y between two datasets, but Y varies drastically based on proofreading level, then it will be challenging to compare Y without ensuring proofreading was done in the exact same way for these two datasets.\nSecond, understanding this sensitivity is important for directing future proofreading efforts. Achieving a “perfect connectome” reconstruction is a comforting goal to strive for, but given finite time and resources, it is worthwhile to consider which efforts will actually affect downstream analysis we are interested in. Increasingly, a single connectomics datasets may be used for addressing many questions by directing proofreading toward particular sub-goals. It is worth understanding which questions can be answered “out-of-the-box” with no additional proofreading after the initial segmentation, and which will require further effort to get a trustworth answer.\n\n\n\nIn this work, we investigate these questions using large-scale reconstructions of mouse visual cortex as testbeds for these ideas. We do not aim to answer these questions holistically for all connectomics datasets and possible connectivity features; such an endeavor would be impossible since the answer is likely to depend greatly on the dataset and the question. Rather, we provide a set of guiding principles for which types of features are likely to be variable, and provide tools for other connectomicists to analyze these questions on their own data.\n\n\n\nWe start by clearly defining a set of connectivity features to consider for this work.\nThen, we extract the complete set of proofreading edits performed on a highly curated column of cortical connectivity in a mouse’s visual cortex. We also map each synapse between neurons in this column to the edits it “depends” on, in the sense that those edits had to happen for a statement about soma \\(i\\) to soma \\(j\\) connectivity to be made.\nUsing this information, we then construct various “sloppy” networks, instantiated by replaying particular edits over the network and omitting others. These networks vary in their degree of completeness (i.e. some measure of how many of the edits were used) from the original segmentation all the way to the final proofread dataset. We also vary the “proofreader model” used to assign edits to “ON” or “OFF” in some ordering."
  },
  {
    "objectID": "introduction.html#background",
    "href": "introduction.html#background",
    "title": "Introduction",
    "section": "",
    "text": "In the effort to understand the brain and its function, connectomics (the mapping of neural wiring diagrams in precise detail, often via electron microscopy) has proven a useful tool. Connectomes are now available for entire larval (Winding et al. 2023) and adult fly brains (Schlegel et al. 2023; Dorkenwald et al. 2023), large regions of mouse visual cortex (MICrONS Consortium et al. 2021), human cortex (Shapson-Coe et al. 2021), and many others. These reconstructions now contain on the order of 100,000s of neurons, and 100s of millions of synapses. This dramatic scaling in accurate connnectome reconstruction has been enabled by a strategy of scalable automatic segmentation of neurons and detection of synapses, followed by laborious human proofreading to correct false merges and extend incomplete pieces of neurons. This effort has been done, understandably, with the intent that we want to be able to trust the resulting reconstruction and believe that it is a fair representation of the neuroanatomy and connectivity of the biological sample."
  },
  {
    "objectID": "introduction.html#the-question",
    "href": "introduction.html#the-question",
    "title": "Introduction",
    "section": "",
    "text": "However, since this manual proofreading is laborious (for instance, there were 2.7 million edits in FlyWire over 4 years as of June 2023) it is worth reflecting on the scientific value of this effort. How consequential was this proofreading on the downstream measures we care about for analysis, such as connection probabilities between cell types? Are some connectivity features stable with proofreading, and others variable? Are certain edits more important than others, in terms of their impact on overal circuit structure? Next, we touch on how others have explored these and related questions in prior work, before motivating our own contributions here."
  },
  {
    "objectID": "introduction.html#prior-work",
    "href": "introduction.html#prior-work",
    "title": "Introduction",
    "section": "",
    "text": "paper from carey and jovo on power\n\nIn Priebe, Vogelstein, and Bock (2013), the authors ask whether one should desire lots of “highly proofread” data, or a smaller amount of partially proofread or noisy data. They frame this question from the perspective of statistical hypothesis testing, asking which approach has higher power for distinguishing network parameters under a toy model. This work is an illustrative case for why, at least in theory, it is possible to actually prefer lots of unproofread data to a smaller amount of curated data. This is a prospect worth considering, especially in light of costly proofreading endeavors. However, in this work the toy model was set up such that unproofread data had no bias, but merely represented a noisier version of a ground truth network. In reality, the unproofread networks we observe in connectomics likely have extreme bias in the way that errors are observed. For instance, one error in segmentation could lead to missing the entire primary axon of a neuron, effectively deleting all of that neurons outputs in one stroke.\n\ncasey paper\nphillip paper mentions variability"
  },
  {
    "objectID": "introduction.html#why-it-matters",
    "href": "introduction.html#why-it-matters",
    "title": "Introduction",
    "section": "",
    "text": "There are several reasons why understanding the variability of connectivity with respect to proofreading is worthwhile for modern connectomics:\nFirst, understanding the sensitivity of connectivity features to proofreading provides clues as to the technical variability associated with an imperfect reconstruction of an underlying biological network. For instance, if one wanted to compare connectivity feature Y between two datasets, but Y varies drastically based on proofreading level, then it will be challenging to compare Y without ensuring proofreading was done in the exact same way for these two datasets.\nSecond, understanding this sensitivity is important for directing future proofreading efforts. Achieving a “perfect connectome” reconstruction is a comforting goal to strive for, but given finite time and resources, it is worthwhile to consider which efforts will actually affect downstream analysis we are interested in. Increasingly, a single connectomics datasets may be used for addressing many questions by directing proofreading toward particular sub-goals. It is worth understanding which questions can be answered “out-of-the-box” with no additional proofreading after the initial segmentation, and which will require further effort to get a trustworth answer."
  },
  {
    "objectID": "introduction.html#our-contribution",
    "href": "introduction.html#our-contribution",
    "title": "Introduction",
    "section": "",
    "text": "In this work, we investigate these questions using large-scale reconstructions of mouse visual cortex as testbeds for these ideas. We do not aim to answer these questions holistically for all connectomics datasets and possible connectivity features; such an endeavor would be impossible since the answer is likely to depend greatly on the dataset and the question. Rather, we provide a set of guiding principles for which types of features are likely to be variable, and provide tools for other connectomicists to analyze these questions on their own data."
  },
  {
    "objectID": "introduction.html#outline",
    "href": "introduction.html#outline",
    "title": "Introduction",
    "section": "",
    "text": "We start by clearly defining a set of connectivity features to consider for this work.\nThen, we extract the complete set of proofreading edits performed on a highly curated column of cortical connectivity in a mouse’s visual cortex. We also map each synapse between neurons in this column to the edits it “depends” on, in the sense that those edits had to happen for a statement about soma \\(i\\) to soma \\(j\\) connectivity to be made.\nUsing this information, we then construct various “sloppy” networks, instantiated by replaying particular edits over the network and omitting others. These networks vary in their degree of completeness (i.e. some measure of how many of the edits were used) from the original segmentation all the way to the final proofread dataset. We also vary the “proofreader model” used to assign edits to “ON” or “OFF” in some ordering."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "docs",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "test.html",
    "href": "test.html",
    "title": "test",
    "section": "",
    "text": "test"
  },
  {
    "objectID": "abstract.html",
    "href": "abstract.html",
    "title": "Abstract",
    "section": "",
    "text": "Historically, connectomes have been extremely labor-intensive to generate, requiring vast amounts of person-hours to annotate electron microscopy (EM) image volumes. Automated reconstruction from EM images using computer vision has recently developed to the point where it is being deployed widely to reconstruct connectomes at unprecedented scales. However, these automated techniques, while impressive, typically still have many errors that require human proofreading of these datasets, which involves linking together fragments of the same neuron which have been separated or separating false-merges of distinct cells. Here, we study the effect that these proofreading modifications have on the cortical wiring diagram of a region of mouse visual cortex. We compute several connectivity metrics such as the probability of two cell types or two specific neurons connecting, and study how this connectivity changes as we artificially replay a specific subset of edits onto the neurons in the volume. We show that while X changes drastically with proofreading, Z and Y metrics are relatively stable after a key subset of edits are applied. Our work lays the foundation for a quantitative assessment of which areas for human or machine proofreading will be the most influential for downstream analysis, guiding future connectomics reconstructions towards answering scientific questions with a practical amount of effort. We also describe how our analysis reveals quantitative estimates of one aspect of the variability associated with connectome reconstruction."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  }
]